{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://opea.dev/wp-content/uploads/sites/9/2024/04/opea-horizontal-color.svg\" alt=\"OPEA Logo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Platform for Enterprise AI (OPEA) ChatQnA Blueprint Deployment (DOCKER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook will instruct you to deploy a RAG chatbot based on OPEA blueprint ChatQnA blueprints using Docker containers\n",
    "\n",
    "<div class=\"warning\" style='padding:0.1em; background-color:#FFD700; color:#000000'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b>READ BEFORE USING THIS NOTEBOOK</b></p>\n",
    "<p style='margin-left:1em;'>\n",
    "Begin at step 2.3 if you already have a pre-provisioned environment. Steps 1 through 2.2 are intended for setting up in a new environment.\n",
    "</p>\n",
    "<p style='margin-bottom:1em; margin-right:1em; text-align:right; font-family:Georgia'> <b>-</b> <i></i>\n",
    "</p></span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro : The Open Platform for Enterprise (OPEA) Project \n",
    "OPEA uses microservices to create high-quality GenAI applications for enterprises, simplifying the scaling and deployment process for production. These microservices leverage a service composer that assembles them into a megaservice thereby creating real-world enterprise AI applications.\n",
    "\n",
    "It’s important to familiarize yourself with the MAIN key elements of OPEA:\n",
    "\n",
    "1. [**GenAIComps**](https://github.com/opea-project/GenAIComps) \n",
    "A collection of microservice components that form a service-based toolkit. Each microservice is designed to perform a specific function or task within the GenAI application architecture. By breaking down the system into these smaller, self-contained services, microservices promote modularity, flexibility, and scalability. This modular approach allows developers to independently develop, deploy, and scale individual components of the application, making it easier to maintain and evolve over time. All of the microservices are containerized, allowing cloud native deployment. Here, you will find contributions to multiple partners/communities to further construction.\n",
    "\n",
    "2. [**GenAIExamples**](https://github.com/opea-project/GenAIExamples)\n",
    "While *GenAIComps* offers a range of microservices, *GenAIExamples* provides practical, deployable solutions to help users implement these services effectively. This repo provides use-case-based applications that demonstrate how the OPEA architecture can be leveraged to build and deploy real-world GenAI applications. In the repo, developers can find practical resources such as Docker Compose files and Kubernetes Helm charts, which help streamline the deployment and scaling of these applications. These resources allow users to quickly set up and run the examples in local or cloud environments, ensuring a seamless experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses Docker as an engine, you can deploy this example on any of the available Cloud providers following [THIS](https://opea-project.github.io/latest/getting-started/README.html) guide . \n",
    "\n",
    "Once your plaftorm is provisioned, follow the below steps to run the example:."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install Docker Engine (Linux)\n",
    "\n",
    "This is the only requirement to have in your environment since OPEA is based on a microservices architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the command below to install docker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://get.docker.com -o get-docker.sh\n",
    "!chmod +x get-docker.sh\n",
    "!sudo sh get-docker.sh --version 26.1\n",
    "!sudo gpasswd -a $USER docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Clone GenAIExamples Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, end-to-end blueprints are provided on OPEA [GenAIExamples repo](https://github.com/opea-project/GenAIExamples), you can see there other examples available like : AgentsQnA, AudioQnA and MultimodalQnA amoung others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/opea-project/GenAIExamples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore the OPEA ChatQnA RAG deployment. As mentioned, it's a microservices blueprint designed for scalability, resilience, and flexibility. In this task you will explore each microservice, the purpose of exploring each microservice is to help you understand how each component contributes to the overall application. This learning path will guide you through the system, illustrating the role of each service and how they work together.\n",
    "\n",
    "Each service can scale individually based on demand, optimizing resources and performance. Additionally, microservices improve fault isolation—if one service fails, it doesn’t disrupt the entire system. This architecture supports efficient maintenance, rapid updates, and adaptability, making it ideal for responding to changing business needs and user demands.\n",
    "\n",
    "Every OPEA configuration is built on three main parts:\n",
    "\n",
    "![microservices-arch](./Images/microservices-arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Megaservice** : Microservice \"orchestrator\". When deploying an end-to-end application with multiple parts involved, there is needed to specify how the flow will be within the microservices. You can learn more from [OPEA documentation](https://github.com/opea-project/GenAIComps?tab=readme-ov-file#megaservice)\n",
    "\n",
    "- **Gateway** : A gateway is the interface for users to access to the `megaservice` It acts as the entry point for incoming requests, routing them to the appropriate Microservices within the megaservice architecture.\n",
    "\n",
    "- **Microservice** : Each individual microservice part of the end-to-end application like : **embeddings**, **retrievers**, **LLM** and **vector databases** among others.\n",
    "\n",
    "You can see below the architecture you will be deploying `ChatQnA`\n",
    "\n",
    "![\"opea-rag\"](./Images/opea_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Set your enviroment variables \n",
    "\n",
    "Each microservice has a set of configurable variables that will specifcy the containter behaviour.For this example, we can specify which Models will be used by the `Embedding`, `LLM` and `reranker`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Start the containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPEA provides containers already created to allow users to build their own blueprint picking each components based on your needs. This is done through a docker compose file, for this example, we will be using the file available on the repo to use `Milvus` as Vector Data base. You can explore some `docker compose` files provisioned for ChatQnA [HERE](https://github.com/opea-project/GenAIExamples/tree/main/ChatQnA/docker_compose), or you can build your own to use the multiple available components on [GenAIComps](https://github.com/opea-project/GenAIComps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `docker compose` to deploy the Blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile deploy_chatqna_docker.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# Set environment variables\n",
    "export HUGGINGFACEHUB_API_TOKEN=\"Your Hugging Face Token\"  # Required for Hugging Face authentication\n",
    "export EMBEDDING_MODEL_ID=\"BAAI/bge-base-en-v1.5\"          # Embedding model ID\n",
    "export RERANK_MODEL_ID=\"BAAI/bge-reranker-base\"            # Reranker model ID\n",
    "export LLM_MODEL_ID=\"Qwen/Qwen3-8B\"                        # LLM model ID (e.g., Qwen 8B)\n",
    "# Uncomment the following line to use a different LLM model\n",
    "# export LLM_MODEL_ID=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "export host_ip=$(hostname -I | awk '{print $1}')            # Get the host IP address\n",
    "\n",
    "# Set proxy variables if needed (leave empty if not required)\n",
    "export http_proxy=\"\"\n",
    "export https_proxy=\"\"\n",
    "export no_proxy=\"\"\n",
    "\n",
    "# Source the set_env.sh file if it contains additional environment variables\n",
    "cd GenAIExamples/ChatQnA/docker_compose/intel/cpu/xeon/\n",
    "source set_env.sh\n",
    "\n",
    "# Run Docker Compose with the -E flag to preserve environment variables\n",
    "# Uncomment the following line to use a different compose file\n",
    "# sudo -E docker compose -f compose_tgi.yaml up -d\n",
    "sudo -E docker compose -f compose.yaml up -d\n",
    "\n",
    "# Print environment variables for debugging\n",
    "echo \"Environment variables set:\"\n",
    "env | grep -E 'HUGGINGFACEHUB_API_TOKEN|MODEL_ID|host_ip|INDEX_NAME|LOGFLAG|proxy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any containers are running\n",
    "!sudo docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: The cell below deploys all required Docker containers. This process can take 5+ minutes to complete. Please be patient and wait for the cell to finish execution before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!chmod +x deploy_chatqna_docker.sh\n",
    "!sudo ./deploy_chatqna_docker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Verify Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should be able to see 6 containers\n",
    "!sudo docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example runs the `LLM` locally, so you might have to wait ~10 min to allow the LLM container to download the model from Hugging Face (this is why you provided the key before).\n",
    "\n",
    "### Check vLLM Service Connection Status\n",
    "\n",
    "**IMPORTANT: Re-run this cell repeatedly until you see \"completed\" in the output. This indicates that the vLLM service is fully initialized and ready for use. Once \"completed\" appears, the service is ready and you can proceed to the next step**.\n",
    "\n",
    "**Note: Initial service startup may take 1-2 minutes depending on your environment**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check tgi-service - Wait Until you see tgi-service Conneccted\n",
    "!sudo docker logs vllm-service 2>&1 | grep complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test your application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to get your VM's public IP\n",
    "!curl -s http://ifconfig.me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatQnA is an end-to-end blueprint, which means that in addition to the components mentioned before, you will also have an UI to interact with your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Use the UI \n",
    "\n",
    "## The Role of Nginx in Your Setup\n",
    "\n",
    "Nginx is functioning as a **reverse proxy** in your architecture. Here's what it's doing:\n",
    "\n",
    "1. **Front Door**: Nginx listens on port 80 (standard HTTP port) and is the entry point for all web traffic\n",
    "2. **Traffic Routing**: It receives requests from users and routes them to the appropriate internal services\n",
    "3. **UI Serving**: It forwards requests to your ChatQnA UI service on port 5173\n",
    "4. **API Routing**: It routes API calls to your backend services (like the model servers)\n",
    "5. **Load Balancing**: If you had multiple instances of services, Nginx could distribute traffic between them\n",
    "\n",
    "## Complete Request Flow\n",
    "\n",
    "When you access your public IP:\n",
    "\n",
    "1. Browser makes a request to http://[your-public-ip]/ (implicitly port 80)\n",
    "2. Request arrives at your VM and is directed to the Nginx container\n",
    "3. Nginx processes the request based on its configuration\n",
    "4. For UI requests, Nginx forwards them to the ChatQnA UI container (port 5173)\n",
    "5. UI makes API calls which Nginx routes to appropriate backend services\n",
    "6. Response flows back through the same path to your browser\n",
    "\n",
    "To test on your browser:\n",
    "\n",
    "1. Copy your Jupyter LAB URL, for example:\n",
    "\n",
    "`\"http://{public_ip}:80`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get your public URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your public IP address\n",
    "import subprocess\n",
    "public_ip = subprocess.check_output(\"curl -s http://ifconfig.me\", shell=True).decode().strip()\n",
    "\n",
    "# Create the full URL with port 80 (default Nginx port)\n",
    "url = f\"http://{public_ip}:80\"\n",
    "\n",
    "# Display the URL\n",
    "print(f\"WebUI Chat Interface URL: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You should be able to see the public UI\n",
    "![IMAGE](./Images/chatqna.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** If you deploy this example in your enviroment, you just need to use your `PUBLIC IP` in your browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now interact with the application, we can ask things like **\"What is Nike's revenue in 2023?\"**.\n",
    "\n",
    "Let's now explore each microservice and their behaviour, this will help you understand how they interact and how the RAG example is built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Prompt from the megaservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first test the `MEGASERVICE` microservice, as mentioned, it's responsible to orchestrate the flow.\n",
    "\n",
    "## ChatQnA Backend Service Architecture\n",
    "\n",
    "The ChatQnA backend service functions as the central orchestration layer in the system. Here's its architecture:\n",
    "\n",
    "### Core Components\n",
    "\n",
    "1. **API Layer**: Exposes RESTful endpoints like `/v1/chatqna` for client interactions\n",
    "2. **Session Management**: Maintains conversation history and context between requests\n",
    "3. **Orchestration Engine**: Coordinates the RAG workflow between services\n",
    "\n",
    "### Workflow Process\n",
    "\n",
    "1. When a query arrives, the backend:\n",
    "   - Processes and sanitizes the input\n",
    "   - Determines if retrieval is needed\n",
    "   - Generates embeddings via the TEI service\n",
    "   - Queries Redis Vector DB for relevant documents\n",
    "   - Uses reranking to prioritize the most relevant context\n",
    "   - Constructs a prompt with retrieved context\n",
    "   - Sends the prompt to TGI for generation\n",
    "   - Streams the response back to the client\n",
    "\n",
    "### Integration Points\n",
    "\n",
    "- **Embedding Service**: For converting queries to vector representations\n",
    "- **Vector Database**: For semantic document retrieval\n",
    "- **Language Model**: For generating contextual responses\n",
    "- **Storage Systems**: For persisting conversations and feedback\n",
    "\n",
    "The backend service isolates clients from the complexity of the underlying services, presenting a unified interface while handling all the necessary coordination between specialized components.\n",
    "\n",
    "## Understanding the ChatQnA Architecture and Code\n",
    "\n",
    "The containers shown in your output represent a sophisticated RAG (Retrieval-Augmented Generation) system with multiple specialized services working together:\n",
    "\n",
    "- **Nginx** (port 80): Acts as the reverse proxy for all traffic\n",
    "- **ChatQnA UI** (port 5173): Web interface for user interactions\n",
    "- **Backend Server** (port 8888): Manages the chat logic and orchestration\n",
    "- **Text Generation Inference (TGI)** (port 9009): Handles LLM inference\n",
    "- **Text Embeddings Inference (TEI)** (ports 6006/8808): Creates embeddings and handles reranking\n",
    "- **Redis Vector DB** (port 6379): Stores vector embeddings for retrieval\n",
    "- **Retriever and DataPrep services**: Handle document processing and retrieval\n",
    "\n",
    "This `query_chatqna()` function makes a POST request to the ChatQnA API through Nginx at `/v1/chatqna`. It sends a JSON payload containing the user's query and processes the streamed response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_chatqna(query: str, verbose: bool = True) -> str:\n",
    "    url = \"http://localhost/v1/chatqna\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"messages\": query}\n",
    "\n",
    "    content = \"\"\n",
    "    with requests.post(url, json=payload, headers=headers, stream=True) as response:\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                decoded_line = line.decode(\"utf-8\")\n",
    "                if decoded_line == \"[DONE]\":\n",
    "                    break\n",
    "                \n",
    "                if decoded_line.startswith(\"data:\"):\n",
    "                    data_part = decoded_line[5:].strip()\n",
    "                    \n",
    "                    if data_part.startswith('b\"') and data_part.endswith('\"'):\n",
    "                        clean_text = data_part[2:-1]\n",
    "                    elif data_part.startswith(\"b'\") and data_part.endswith(\"'\"):\n",
    "                        clean_text = data_part[2:-1]\n",
    "                    else:\n",
    "                        clean_text = data_part\n",
    "                    \n",
    "                    clean_text = clean_text.replace('\\\\\"', '\"').replace(\"\\\\'\", \"'\")\n",
    "                    \n",
    "                    content += clean_text\n",
    "                    if verbose:\n",
    "                        print(clean_text, end=\"\", flush=True)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should receive an answer back, verifying that all the services in the RAG flow are working, remember that the microservice is the point of entry and interacts with all the internal services, if you recieve an answer will mean that the blueprint is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query ChatQnA without RAG\n",
    "query = \"What was the revenue of Nike in 2023?\"\n",
    "content = query_chatqna(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You will see that the model couldn’t answer the question because it lacks the necessary context and operates on ***outdated information***. Without up-to-date or relevant data, it’s unable to provide accurate responses. You'll fix this in few minutes by using RAG to allow the model to pull in current, contextually relevant information, ensuring more precise and relevant answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Provide external context before prompting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's help your app to provide context answers!\n",
    "\n",
    "In this step, you will be feeding the application with context information. From the previous query, the model didn't know **\"What is the revenue of Nike in 2023?\"**, affortunately, if we have a document containing that information we could feed our application and have it correclty answered. The process then,  is to start from a document (Nike's revenue PDF), and do the preprocessing needed to make it ready to be stored in a database. As shown, this process primarily involves 3 microservices: `data preparation`, `embeddings` and `vector store`. Let's explore each microservice\n",
    "<p align=\"center\">\n",
    "  <img src=\"./Images/pre_flow.png\" alt=\"Alt text\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Embedding Microservice (Container:chatqna-tei:80)\n",
    "\n",
    "The document has to be ingested in a way that operations like similarities and storage can be done. Documents aren't stored with words, they are stored as `embeddings`.\n",
    "An **embedding** is a numerical representation of an object—such as a word, phrase, or document in a continuous vector space. In the context of natural language processing (NLP), embeddings represent words, sentences, or other pieces of text as a set of numbers (or a \"vector\") that capture their meaning, relationships, and context. By transforming text into this format, embeddings make it easier for machine learning models to understand and work with text data.\n",
    "\n",
    "For example, the following image shows how word embeddings represent words as points in a vector space based on their relationships. Words with similar meanings, like \"king\" and \"queen\" are closer together, and the embedding model captures these connections through vector arithmetic. \n",
    "\n",
    "During training, if the model sees \"king\" often used with \"man\" and \"queen\" with \"woman,\" it learns that \"king\" and \"queen\" relate similarly to \"man\" and \"woman.\" So, it positions these words in ways that reflect gender relationships in language.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./Images/king_vs_queen.png\" alt=\"Alt text\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are a key component for RAG:\n",
    "\n",
    "•\t***Capturing Meaning***: Embeddings represent the semantic relationships between words, allowing RAG models to understand context and nuances in language, enhancing their ability to generate relevant responses.\n",
    "\n",
    "•\t***Dimensionality Reduction***: By converting complex information into fixed-size vectors, embeddings streamline data processing, making RAG systems more efficient and faster.\n",
    "\n",
    "•\t***Improving Model Performance***: Embeddings enable RAG models to generalize better by leveraging semantic similarities, facilitating more accurate information retrieval, and improving the quality of generated content.\n",
    "\n",
    "OPEA provides multiple options to run your embeddings microservice, as detailed in the [OPEA embedding documentation](https://github.com/opea-project/GenAIComps/tree/main/comps/embeddings): \n",
    "\n",
    "In this case, ChatQnA uses [Hugging Face TEI](https://huggingface.co/docs/text-embeddings-inference/en/index) microservice running the embedding model `BAAI/bge-large-en-v1.5` locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the microservice by getting the embedding for the phrase \"What was Deep Learning?\":\n",
    "\n",
    "This command interfaces with the Text Embedding Interface (TEI) microservice running on port 6006. The service transforms natural language into dense vector representations.\n",
    "\n",
    "The service converts the query \"What is Deep Learning?\" into a high-dimensional numerical vector that captures semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:6006/embed -X POST -d \\\n",
    "    '{\"inputs\":\"What is Deep Learning?\"}' -H \\\n",
    "    'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Store the public Nike report for 2023 in the Vector Database (chatqna-data-prep:6007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, this example uses Redis as the vector database. You can find all of the supported alternatives in the [OPEA vector store repository](https://github.com/opea-project/GenAIComps/tree/main/comps)\n",
    "\n",
    "A Vector Database (VDB) is a specialized database designed to store and manage high-dimensional vectors—numeric representations of data points like words, sentences, or images. In AI and machine learning, these vectors are typically embeddings, which capture the meaning and relationships of data in a format that algorithms can process efficiently, as we have shown before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Microservice involved on storing the emebddings to the vdb is the `Dataprep` Microservice. It is responsible for preparing data in a digestible format for the application, converting it to embeddings (using the embedding microservice) and loading it to the database. This service preprocesses/transforms the data, making sure it is clean, organized, and suitable for further processing.  \n",
    "\n",
    "Specifically, this microservice receives data (such as documents), processes it by breaking it into chunks, sends it to the embedding microservice, and stores these vectors in the vector database. The microservice's functionality may depend on the specific vector database being used, as each database has its own requirements for data formatting\n",
    "\n",
    "To test it and help the model answer the initial question **What was Nike revenue in 2023?**, you will need to upload a context file (revenue report) to be processed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Execute the following command to download a sample [Nike revenue report](https://github.com/opea-project/GenAIComps/tree/v1.1/comps/retrievers/redis/data) to the nginx pod (if you are no longer logged in to the NGinx pod, be sure to use the above command to log in again):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest your data for your RAG. In this case NIke's revenue document\n",
    "\n",
    "!wget https://raw.githubusercontent.com/opea-project/GenAIComps/v1.1/comps/retrievers/redis/data/nke-10k-2023.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run the ingestion function on the `dataprep` microservice.\n",
    "This command uploads the document to the DataPrep ingestion service running on port 6007. The service processes the PDF by extracting text, chunking content, generating vector embeddings, and storing these in the vector database—creating searchable knowledge that the ChatQnA system can retrieve during conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"http://localhost:6007/v1/dataprep/ingest\" \\\n",
    "     -H \"Content-Type: multipart/form-data\" \\\n",
    "     -F \"files=@./nke-10k-2023.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"http://localhost:6007/v1/dataprep/ingest\" \\\n",
    "    -H \"Content-Type: multipart/form-data\" \\\n",
    "    -F \"files=@./nke-10k-2023.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run the get function to retrieve all documents ingested on the vdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if the file was successfully updated\n",
    "!curl -X POST \"http://localhost:6007/v1/dataprep/get\" \\\n",
    "     -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Test if your RAG application provide answer based on context (Nike revenue in 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run your query (Directly to the LLM)\n",
    "This command sends a direct query to the LLM service running on port 9009, bypassing the RAG pipeline. It submits a completion request to the asking a question about uploaded document, with geenrates the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:9009/v1/chat/completions \\\n",
    "  -X POST \\\n",
    "  -d '{\"model\": \"'\"$LLM_MODEL_ID\"'\", \"messages\": [{\"role\": \"user\", \"content\": \"What was the revenue of Nike in 2023?\"}], \"max_tokens\":100}' \\\n",
    "  -H 'Content-Type: application/json' | jq -r '.choices[0].message.content'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the model doesn't answered correctly, but WHY? \n",
    "\n",
    "The answer is simple, the LLM isn't re trained with the context, the information you added in the previous step (Nike revenue) is on the vector database. This information is given as a context (new prompt) to the LLM when is prompted from the entry point `megaservice` which will start the process of recieving the answer, finding the most similar documents (chunks) on the vector database using the `retriever` microservice and finally creating a new prompt that is ingested to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Query ChatQnA WITH RAG\n",
    "\n",
    "This code invokes the `query_chatqna()` function to process the question through the complete RAG pipeline. Unlike direct LLM calls, this function handles streaming the response from the ChatQnA service, which retrieves relevant financial data from the vector database, provides it as context to the LLM, and returns a factually-grounded answer about the question asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was the revenue of Nike in 2023?\"\n",
    "content = query_chatqna(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Delete documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"http://localhost:6007/v1/dataprep/delete\" \\\n",
    "     -d '{\"file_path\": \"all\"}' \\\n",
    "     -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Delete Containers and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd GenAIExamples/ChatQnA/docker_compose/intel/cpu/xeon/ && sudo -E docker compose -f compose.yaml down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "The ChatQnA system demonstrates a modern microservices architecture for Retrieval Augmented Generation. By separating text embedding, document ingestion, LLM inference, and orchestration into discrete services, the system achieves modularity and scalability. This architecture allows developers to interact with individual components for specific tasks or leverage the integrated RAG pipeline for knowledge-grounded responses about complex documents like financial reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Be involved with OPEA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPEA is an open source project that welcomes contributions in many forms. Whether you're interested in fixing bugs, adding new GenAI components, improving documentation, or sharing your unique use cases, there are numerous ways to get involved and make a meaningful impact. \n",
    "\n",
    "We’re excited to have you on board and look forward to the contributions you'll bring to the OPEA platform. \n",
    "\n",
    "For detailed instructions on how to contribute, please check out our [Contributing Documentation](https://opea-project.github.io/latest/community/CONTRIBUTING.html).\n",
    "\n",
    "[Follow the project](https://github.com/opea-project) and stay tuned for new events!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
